{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64586122",
   "metadata": {},
   "source": [
    "# Analyse du data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f8c5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BTC_Silver\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab2f89ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+-------+--------------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|          open_time|    open|    high|     low|   close| volume|          close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------------------+--------+--------+--------+--------+-------+--------------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|2026-01-19 09:53:00|93046.35|93046.35|92997.65| 93001.8|9.10532|2026-01-19 09:53:...|    847027.2616431|            2024|              2.08878|        194272.7849138|     0|\n",
      "|2026-01-19 09:54:00| 93001.8|93022.23|93001.79|93014.94|5.62556|2026-01-19 09:54:...|    523245.8306426|            1064|              3.42885|        318907.7899883|     0|\n",
      "|2026-01-19 09:55:00|93014.95|93042.27|93014.95|93022.36|3.74306|2026-01-19 09:55:...|    348227.1952126|            1863|              1.16913|        108765.9135497|     0|\n",
      "|2026-01-19 09:56:00|93022.36|93042.25|93019.76|93019.77|6.90779|2026-01-19 09:56:...|    642674.4542098|            1486|              0.84542|         78650.4419022|     0|\n",
      "|2026-01-19 09:57:00|93019.77|93019.77|93016.22|93016.23|1.07452|2026-01-19 09:57:...|     99947.9724656|             333|              0.83665|         77822.0492907|     0|\n",
      "+-------------------+--------+--------+--------+--------+-------+--------------------+------------------+----------------+---------------------+----------------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Lecture du fichier Parquet Bronze\n",
    "bronze_path = \"../data/bronze/btc_minute_data.parquet\"\n",
    "df_bronze = spark.read.parquet(bronze_path)\n",
    "\n",
    "# Afficher un aperçu\n",
    "df_bronze.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7bd24b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: timestamp_ntz (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: timestamp_ntz (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: long (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a7247f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open_time',\n",
       " 'open',\n",
       " 'high',\n",
       " 'low',\n",
       " 'close',\n",
       " 'volume',\n",
       " 'close_time',\n",
       " 'quote_asset_volume',\n",
       " 'number_of_trades',\n",
       " 'taker_buy_base_volume',\n",
       " 'taker_buy_quote_volume',\n",
       " 'ignore']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bronze.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d514457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|summary|              open|              high|               low|             close|           volume|quote_asset_volume|  number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|  count|               600|               600|               600|               600|              600|               600|               600|                  600|                   600|   600|\n",
      "|   mean| 93070.08556666668| 93084.20378333335| 93055.98494999995|  93070.1422166667|5.902528833333327| 549296.6430280326|1694.1466666666668|    2.621530233333333|     243988.0310935778|   0.0|\n",
      "| stddev|119.90986944226844|118.71267837031895|120.47082563998032|119.90714518894747|7.596808147729382| 706672.9781948837|1193.0722081998445|   3.3902028270286806|    315545.41897675494|   0.0|\n",
      "|    min|          92725.61|          92763.39|           92700.0|          92725.61|           0.3011|     28010.4908646|                86|              0.05203|          4841.0483547|     0|\n",
      "|    max|           93382.7|           93420.0|          93358.67|           93382.7|        114.47682|1.06493495676361E7|              7537|             40.04339|       3726125.6962924|     0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bronze.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82ae94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+\n",
      "|open_time|open|high|low|close|volume|close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+\n",
      "|        0|   0|   0|  0|    0|     0|         0|                 0|               0|                    0|                     0|     0|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1363)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:356)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:45739\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/21 14:22:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:45739\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "df_bronze.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "      for c in df_bronze.columns\n",
    "      ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a4d7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres des lingne: 600\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombres des lingne:\", df_bronze.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2acf5cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes uniques :600\n"
     ]
    }
   ],
   "source": [
    "uniques_lignes = df_bronze.dropDuplicates().count()\n",
    "print(f\"Nombre de lignes uniques :{uniques_lignes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a6e5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decaler la colonne close de 10 lignes\n",
    "\n",
    "window = Window.orderBy(\"open_time\")\n",
    "\n",
    "df_silver = df_bronze.withColumn(\"close_t_plus_10\", F.lead(\"close\", 10).over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c78339e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+\n",
      "|          open_time|   close|           return_1m|\n",
      "+-------------------+--------+--------------------+\n",
      "|2026-01-19 09:53:00| 93001.8|                NULL|\n",
      "|2026-01-19 09:54:00|93014.94|1.412875879821618...|\n",
      "|2026-01-19 09:55:00|93022.36| 7.97721312296525E-5|\n",
      "|2026-01-19 09:56:00|93019.77|-2.78427681258195...|\n",
      "|2026-01-19 09:57:00|93016.23|-3.80564260695134...|\n",
      "+-------------------+--------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Calcule du return \n",
    "window = Window.orderBy(\"open_time\")\n",
    "\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"return_1m\",\n",
    "    (F.col(\"close\") - F.lag(\"close\", 1).over(window)) / F.lag(\"close\", 1).over(window)\n",
    ")\n",
    "\n",
    "df_silver.select(\"open_time\", \"close\", \"return_1m\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "109de06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  la moyenne des prix de clôture sur les 5 et 10 dernières minutes \n",
    "\n",
    "window_5 = Window.orderBy(\"open_time\").rowsBetween(-4, 0)\n",
    "window_10 = Window.orderBy(\"open_time\").rowsBetween(-9, 0)\n",
    "  \n",
    "df_silver = df_silver.withColumn(\"MA_5\", F.avg(\"close\").over(window_5))\n",
    "df_silver = df_silver.withColumn(\"MA_10\", F.avg(\"close\").over(window_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2683b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume et intensité de trading\n",
    "\n",
    "df_silver = df_silver.withColumn(\"taker_ratio\", \n",
    "      F.col(\"taker_buy_base_volume\") / F.col(\"volume\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66afdc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "|          open_time|    open|    high|     low|   close|  volume|          close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|close_t_plus_10|           return_1m|             MA_5|            MA_10|         taker_ratio|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "|2026-01-19 09:53:00|93046.35|93046.35|92997.65| 93001.8| 9.10532|2026-01-19 09:53:...|    847027.2616431|            2024|              2.08878|        194272.7849138|     0|       92965.51|                NULL|          93001.8|          93001.8| 0.22940215170911069|\n",
      "|2026-01-19 09:54:00| 93001.8|93022.23|93001.79|93014.94| 5.62556|2026-01-19 09:54:...|    523245.8306426|            1064|              3.42885|        318907.7899883|     0|       92975.79|1.412875879821618...|         93008.37|         93008.37|  0.6095126529625495|\n",
      "|2026-01-19 09:55:00|93014.95|93042.27|93014.95|93022.36| 3.74306|2026-01-19 09:55:...|    348227.1952126|            1863|              1.16913|        108765.9135497|     0|       92965.51| 7.97721312296525E-5|93013.03333333333|93013.03333333333| 0.31234604842027647|\n",
      "|2026-01-19 09:56:00|93022.36|93042.25|93019.76|93019.77| 6.90779|2026-01-19 09:56:...|    642674.4542098|            1486|              0.84542|         78650.4419022|     0|        92900.0|-2.78427681258195...|       93014.7175|       93014.7175| 0.12238646513573805|\n",
      "|2026-01-19 09:57:00|93019.77|93019.77|93016.22|93016.23| 1.07452|2026-01-19 09:57:...|     99947.9724656|             333|              0.83665|         77822.0492907|     0|       92949.84|-3.80564260695134...|93015.01999999999|93015.01999999999|  0.7786267356587128|\n",
      "|2026-01-19 09:58:00|93016.22| 93046.0|93016.22|93045.99|  1.7627|2026-01-19 09:58:...|    163978.8458685|            1091|              1.51888|        141294.7228396|     0|        92910.0|3.199441645829907E-4|        93023.858|93020.18166666666|  0.8616781074488001|\n",
      "|2026-01-19 09:59:00| 93046.0|93060.07|93045.99|93060.06| 3.78297|2026-01-19 09:59:...|    352010.2429518|             422|              3.45572|         321558.591768|     0|       92900.01|1.512155440550681...|        93032.882|93025.87857142856|  0.9134938950084193|\n",
      "|2026-01-19 10:00:00|93060.06|93060.07|93014.94|93014.94| 3.45839|2026-01-19 10:00:...|    321784.6178058|            3150|              0.21597|         20093.5206898|     0|       92909.99|-4.84848172244842...|        93031.398|93024.51124999998|0.062448133379983166|\n",
      "|2026-01-19 10:01:00|93014.94|93014.95|92985.27|92987.47|10.88765|2026-01-19 10:01:...|   1012538.7568607|            1008|              2.07223|        192745.1402557|     0|        92943.1|-2.95328900927110...|93024.93800000001|93020.39555555553| 0.19032849145591563|\n",
      "|2026-01-19 10:02:00|92987.46|92987.46|92966.41|92966.41| 4.03649|2026-01-19 10:02:...|    375282.0164132|            1212|              0.67966|         63187.1073354|     0|       92945.42|-2.26482127108068...|        93014.974|93014.99699999999| 0.16837896291084584|\n",
      "|2026-01-19 10:03:00| 92966.4|92966.41| 92965.5|92965.51| 1.05291|2026-01-19 10:03:...|     97884.3235791|             366|              0.26409|         24551.2622829|     0|       92977.99|  -9.680915935215E-6|        92998.878|        93011.368| 0.25081915833262103|\n",
      "|2026-01-19 10:04:00|92965.51|92975.84| 92965.5|92975.79| 2.67554|2026-01-19 10:04:...|     248750.488412|             541|              1.81923|        169135.4773166|     0|       92968.03|1.105786436281459...|        92982.024|93007.45300000001|  0.6799487206320967|\n",
      "|2026-01-19 10:05:00|92975.79|92979.57| 92965.5|92965.51| 2.53975|2026-01-19 10:05:...|    236132.9290884|            1405|              0.86971|         80861.8314997|     0|       92997.23|-1.10566417343685...|        92972.138|93001.76800000001|  0.3424392164583128|\n",
      "|2026-01-19 10:06:00| 92965.5|92965.51| 92900.0| 92900.0|  7.5464|2026-01-19 10:06:...|    701373.9951396|            2415|              1.58385|        147241.5809635|     0|       93024.24|-7.04669936194560...|        92954.644|92989.79100000001| 0.20988153291635747|\n",
      "|2026-01-19 10:07:00| 92900.0|92949.84| 92900.0|92949.84| 2.44715|2026-01-19 10:07:...|    227406.0125245|            1828|              2.17301|        201928.4723135|     0|       93024.25|5.364908503767116E-4|         92951.33|        92983.152|  0.8879758085936702|\n",
      "|2026-01-19 10:08:00|92949.84| 92950.0| 92910.0| 92910.0| 4.54348|2026-01-19 10:08:...|    422227.8786078|            1262|              0.39185|         36422.0337105|     0|       93025.31|-4.28618274114258...|        92940.228|        92969.553| 0.08624446459542025|\n",
      "|2026-01-19 10:09:00|92910.01|92910.01| 92900.0|92900.01| 2.31613|2026-01-19 10:09:...|     215173.770729|             664|              1.54812|        143823.0279947|     0|       93034.19|-1.07523409751428...|        92925.072|        92953.548|  0.6684080772668202|\n",
      "|2026-01-19 10:10:00| 92900.0| 92910.0| 92900.0|92909.99| 4.73795|2026-01-19 10:10:...|    440161.4577132|             528|              3.86697|        359244.9692621|     0|        93017.5|1.074273296634788...|        92913.968|92943.05299999999|  0.8161694403697802|\n",
      "|2026-01-19 10:11:00| 92910.0| 92943.1|92909.99| 92943.1| 2.63853|2026-01-19 10:11:...|    245170.0060038|             881|              2.43927|        226653.9043814|     0|       93048.31|3.563664144189508...|92922.58799999999|        92938.616|  0.9244806767404579|\n",
      "|2026-01-19 10:12:00| 92943.1|92945.43| 92943.1|92945.42| 0.55559|2026-01-19 10:12:...|     51639.4791458|             312|              0.37095|          34478.036797|     0|        93070.0|2.496150870793456...|        92921.704|        92936.517|  0.6676686045465181|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df_silver.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a777058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "|open_time|open|high|low|close|volume|close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|close_t_plus_10|return_1m|MA_5|MA_10|taker_ratio|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "|        0|   0|   0|  0|    0|     0|         0|                 0|               0|                    0|                     0|     0|             10|        1|   0|    0|          0|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df_silver.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "      for c in df_silver.columns\n",
    "      ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dbaca29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where target is null \n",
    "df_silver = df_silver.na.drop(subset=[\"close_t_plus_10\", \"return_1m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0361eb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "|open_time|open|high|low|close|volume|close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|close_t_plus_10|return_1m|MA_5|MA_10|taker_ratio|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "|        0|   0|   0|  0|    0|     0|         0|                 0|               0|                    0|                     0|     0|              0|        0|   0|    0|          0|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------+----------------------+------+---------------+---------+----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df_silver.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "      for c in df_silver.columns\n",
    "      ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7095348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes uniques :589\n"
     ]
    }
   ],
   "source": [
    "uniques_lignes = df_silver.dropDuplicates().count()\n",
    "print(f\"Nombre de lignes uniques :{uniques_lignes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc7c8103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 92998.27, Q3: 93146.1, IQR: 147.83000000000175\n",
      "Bornes: [92776.525, 93367.845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'outliers: 8\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+----------+\n",
      "|          open_time|    open|    high|     low|   close|  volume|          close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|close_t_plus_10|           return_1m|             MA_5|            MA_10|         taker_ratio|is_outlier|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+----------+\n",
      "|2026-01-19 14:26:00|92813.52|92842.91|92813.51|92838.05| 4.28239|2026-01-19 14:26:...|    397495.9354988|            1525|              3.20924|        297873.0778412|     0|       92767.13|2.642933917386048...|         92885.76|        92901.122|  0.7494039543339116|      true|\n",
      "|2026-01-19 14:27:00|92838.05|92859.16|92838.05|92859.16| 8.45306|2026-01-19 14:27:...|    784854.7736765|            1515|              0.78453|         72842.6196928|     0|       92725.61|2.273852154369957...|92862.24399999999|92900.60500000001| 0.09281017761615319|      true|\n",
      "|2026-01-19 14:28:00|92859.15|92879.61|92859.15|92879.61| 5.22003|2026-01-19 14:28:...|    484735.2155358|             758|              4.86734|         451983.288138|     0|       92736.43|2.202259852447199...|92851.03799999999|92898.56700000001|  0.9324352542035199|      true|\n",
      "|2026-01-19 14:29:00| 92879.6|92879.61|92862.72|92871.24|  6.1201|2026-01-19 14:29:...|    568366.2578865|            1392|              2.41203|        223996.7426042|     0|       92763.39|-9.01166574665348...|92852.31599999999|        92894.592|  0.3941161092139018|      true|\n",
      "|2026-01-19 14:35:00|92925.17|92925.18| 92854.1|92854.11|14.44045|2026-01-19 14:35:...|   1341289.5871907|            2520|              0.26574|         24688.0906084|     0|       92762.27|-7.64808849442018...|92912.38399999999|92890.89799999999| 0.01840247360712443|      true|\n",
      "|2026-01-19 17:03:00|93231.97|93231.99|93150.01|93150.01| 9.15127|2026-01-19 17:03:...|    852866.1131732|            2923|              0.51524|         48005.2953766|     0|        93382.7|-8.79097588520401...|93254.68000000001|93301.03899999999|0.056302567840310694|      true|\n",
      "|2026-01-19 17:05:00|93150.01|93208.03| 93150.0|93208.02| 4.09364|2026-01-19 17:05:...|     381481.604028|            2197|               3.3407|        311320.2460519|     0|       93372.56|6.227589240195392E-4|        93206.988|        93266.165|  0.8160707829706569|      true|\n",
      "|2026-01-19 17:16:00|93372.56|93379.52|93355.32|93355.32| 4.78824|2026-01-19 17:16:...|    447099.2281237|             889|              0.67947|         63441.4436849|     0|       93379.44|-1.84636685552914...|        93362.916|        93322.255| 0.14190391459074733|      true|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+------+---------------+--------------------+-----------------+-----------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculer les quartiles et l'IQR\n",
    "quantiles = df_silver.select(\n",
    "    F.expr('percentile_approx(close_t_plus_10, 0.25)').alias('Q1'),\n",
    "    F.expr('percentile_approx(close_t_plus_10, 0.75)').alias('Q3')\n",
    ").collect()[0]\n",
    "\n",
    "Q1 = quantiles['Q1']\n",
    "Q3 = quantiles['Q3']\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Définir les bornes\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "print(f\"Bornes: [{lower_bound}, {upper_bound}]\")\n",
    "\n",
    "# Détecter les outliers\n",
    "df_with_outliers = df_silver.withColumn(\n",
    "    'is_outlier',\n",
    "    F.when(\n",
    "        (F.col('close_t_plus_10') < lower_bound) | (F.col('close_t_plus_10') > upper_bound),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Compter les outliers\n",
    "outlier_count = df_with_outliers.filter(F.col('is_outlier') == True).count()\n",
    "print(f\"Nombre d'outliers: {outlier_count}\")\n",
    "\n",
    "# Afficher les outliers\n",
    "df_with_outliers.filter(F.col('is_outlier') == True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "108e478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "|          open_time|    open|    high|     low|   close|  volume|          close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|close_t_plus_10|           return_1m|             MA_5|            MA_10|         taker_ratio|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "|2026-01-19 09:54:00| 93001.8|93022.23|93001.79|93014.94| 5.62556|2026-01-19 09:54:...|    523245.8306426|            1064|              3.42885|        318907.7899883|       92975.79|1.412875879821618...|         93008.37|         93008.37|  0.6095126529625495|\n",
      "|2026-01-19 09:55:00|93014.95|93042.27|93014.95|93022.36| 3.74306|2026-01-19 09:55:...|    348227.1952126|            1863|              1.16913|        108765.9135497|       92965.51| 7.97721312296525E-5|93013.03333333333|93013.03333333333| 0.31234604842027647|\n",
      "|2026-01-19 09:56:00|93022.36|93042.25|93019.76|93019.77| 6.90779|2026-01-19 09:56:...|    642674.4542098|            1486|              0.84542|         78650.4419022|        92900.0|-2.78427681258195...|       93014.7175|       93014.7175| 0.12238646513573805|\n",
      "|2026-01-19 09:57:00|93019.77|93019.77|93016.22|93016.23| 1.07452|2026-01-19 09:57:...|     99947.9724656|             333|              0.83665|         77822.0492907|       92949.84|-3.80564260695134...|93015.01999999999|93015.01999999999|  0.7786267356587128|\n",
      "|2026-01-19 09:58:00|93016.22| 93046.0|93016.22|93045.99|  1.7627|2026-01-19 09:58:...|    163978.8458685|            1091|              1.51888|        141294.7228396|        92910.0|3.199441645829907E-4|        93023.858|93020.18166666666|  0.8616781074488001|\n",
      "|2026-01-19 09:59:00| 93046.0|93060.07|93045.99|93060.06| 3.78297|2026-01-19 09:59:...|    352010.2429518|             422|              3.45572|         321558.591768|       92900.01|1.512155440550681...|        93032.882|93025.87857142856|  0.9134938950084193|\n",
      "|2026-01-19 10:00:00|93060.06|93060.07|93014.94|93014.94| 3.45839|2026-01-19 10:00:...|    321784.6178058|            3150|              0.21597|         20093.5206898|       92909.99|-4.84848172244842...|        93031.398|93024.51124999998|0.062448133379983166|\n",
      "|2026-01-19 10:01:00|93014.94|93014.95|92985.27|92987.47|10.88765|2026-01-19 10:01:...|   1012538.7568607|            1008|              2.07223|        192745.1402557|        92943.1|-2.95328900927110...|93024.93800000001|93020.39555555553| 0.19032849145591563|\n",
      "|2026-01-19 10:02:00|92987.46|92987.46|92966.41|92966.41| 4.03649|2026-01-19 10:02:...|    375282.0164132|            1212|              0.67966|         63187.1073354|       92945.42|-2.26482127108068...|        93014.974|93014.99699999999| 0.16837896291084584|\n",
      "|2026-01-19 10:03:00| 92966.4|92966.41| 92965.5|92965.51| 1.05291|2026-01-19 10:03:...|     97884.3235791|             366|              0.26409|         24551.2622829|       92977.99|  -9.680915935215E-6|        92998.878|        93011.368| 0.25081915833262103|\n",
      "|2026-01-19 10:04:00|92965.51|92975.84| 92965.5|92975.79| 2.67554|2026-01-19 10:04:...|     248750.488412|             541|              1.81923|        169135.4773166|       92968.03|1.105786436281459...|        92982.024|93007.45300000001|  0.6799487206320967|\n",
      "|2026-01-19 10:05:00|92975.79|92979.57| 92965.5|92965.51| 2.53975|2026-01-19 10:05:...|    236132.9290884|            1405|              0.86971|         80861.8314997|       92997.23|-1.10566417343685...|        92972.138|93001.76800000001|  0.3424392164583128|\n",
      "|2026-01-19 10:06:00| 92965.5|92965.51| 92900.0| 92900.0|  7.5464|2026-01-19 10:06:...|    701373.9951396|            2415|              1.58385|        147241.5809635|       93024.24|-7.04669936194560...|        92954.644|92989.79100000001| 0.20988153291635747|\n",
      "|2026-01-19 10:07:00| 92900.0|92949.84| 92900.0|92949.84| 2.44715|2026-01-19 10:07:...|    227406.0125245|            1828|              2.17301|        201928.4723135|       93024.25|5.364908503767116E-4|         92951.33|        92983.152|  0.8879758085936702|\n",
      "|2026-01-19 10:08:00|92949.84| 92950.0| 92910.0| 92910.0| 4.54348|2026-01-19 10:08:...|    422227.8786078|            1262|              0.39185|         36422.0337105|       93025.31|-4.28618274114258...|        92940.228|        92969.553| 0.08624446459542025|\n",
      "|2026-01-19 10:09:00|92910.01|92910.01| 92900.0|92900.01| 2.31613|2026-01-19 10:09:...|     215173.770729|             664|              1.54812|        143823.0279947|       93034.19|-1.07523409751428...|        92925.072|        92953.548|  0.6684080772668202|\n",
      "|2026-01-19 10:10:00| 92900.0| 92910.0| 92900.0|92909.99| 4.73795|2026-01-19 10:10:...|    440161.4577132|             528|              3.86697|        359244.9692621|        93017.5|1.074273296634788...|        92913.968|92943.05299999999|  0.8161694403697802|\n",
      "|2026-01-19 10:11:00| 92910.0| 92943.1|92909.99| 92943.1| 2.63853|2026-01-19 10:11:...|    245170.0060038|             881|              2.43927|        226653.9043814|       93048.31|3.563664144189508...|92922.58799999999|        92938.616|  0.9244806767404579|\n",
      "|2026-01-19 10:12:00| 92943.1|92945.43| 92943.1|92945.42| 0.55559|2026-01-19 10:12:...|     51639.4791458|             312|              0.37095|          34478.036797|        93070.0|2.496150870793456...|        92921.704|        92936.517|  0.6676686045465181|\n",
      "|2026-01-19 10:13:00|92945.43| 92978.0|92945.42|92977.99| 5.29276|2026-01-19 10:13:...|    492014.6143876|            1076|              3.06376|        284803.6438506|        93078.9| 3.50420709272248E-4|        92935.302|        92937.765|  0.5788586673115728|\n",
      "+-------------------+--------+--------+--------+--------+--------+--------------------+------------------+----------------+---------------------+----------------------+---------------+--------------------+-----------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:22:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df_silver = df_silver.drop('ignore')\n",
    "df_silver.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 14:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 14:23:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:45739\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/21 14:23:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1363)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:356)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.255.255.254:45739\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/21 14:23:10 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "df_silver.coalesce(1)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .parquet(\"../data/silver/silver_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
